\documentclass{article}
\title{\textbf{Reinforcement Learning}}
\author{Ash Bellett}
\date{}
\usepackage{titling}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{tikz}
\renewcommand\maketitlehooka{\null\mbox{}\vfill}
\renewcommand\maketitlehookd{\vfill\null}
\begin{document}
\clearpage
\maketitle
\thispagestyle{empty}
\setcounter{page}{0}
\newpage
\tableofcontents
\setcounter{page}{0}
\newpage

\section{Markov processes}

\subsection{Markov process}
A Markov process is described by:
\begin{itemize}
    \item a finite set of states $ \mathcal{S} $
    \item a state transition probability matrix $ P_{s,s'} $ where $ s, s' \in \mathcal{S} $
\end{itemize}
    A sequence of random variables $ \{S_t \in \mathcal{S} \} $ follow the Markov property: the next state $ S_{t+1} $ depends only on the current state $ S_t $:
\[ \text{P}(S_{t+1} = s' | S_t = s_t, S_{t-1} = s_{t-1}, S_{t-2} = s_{t-2}, ...) = \text{P}(S_{t+1} = s' | S_t = s_t) \]
The state transition probability matrix $ P_{s,s'} $ gives the probability of transitioning from state $ s $ to state $ s' $:
\[ P_{s,s'} = \text{P}(S_{t+1} = s' | S_t = s) \]
% TODO: Add example state diagram with labels
% TODO: Explain labels using state space and state transition probability matrix

\subsection{Markov reward process}
A Markov reward process is described by:
\begin{itemize}
    \item a finite set of states $ \mathcal{S} $
    \item a finite set of rewards $ \mathcal{R} \subset \mathbb{R} $
    \item a state transition probability matrix $ P_{s,s'} $
    \item a reward function $r: \mathcal{S} \to \mathcal{R} $
    \item a discount factor $ \gamma \in [0, 1] $
\end{itemize}    
The reward function $ r $ maps a state $  s \in \mathcal{S} $ to a reward $ R_t \in \mathcal{R} $:
\[ R_t = r(s | S_t = s) \]
The next expected reward $ R_s $ at state $ s $ is defined as:
\begin{equation*}
\begin{split}
R_s & =\text{E}(R_{t+1} | S_t = s) \\
 & = \sum_{s' \in \mathcal{S}} r(s') P_{s,s'}
\end{split}
\end{equation*}
The return $ G_t \in \mathbb{R} $ is a sum of attenuated future rewards represented as a geometric series:
\begin{equation*}
\begin{split}
G_t & = R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + ... \\
 & = \sum_{k = 0}^{\infty} \gamma^{k} R_{t+k+1}
\end{split}
\end{equation*}
The return can be rewritten as:
\begin{equation*}
\begin{split}
G_t & = R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + ... \\
 & = R_{t+1} + \gamma ( R_{t+2} + \gamma R_{t+3} + ...) \\
 & = R_{t+1} + \gamma G_{t+1}
\end{split}
\end{equation*}
The state value function $ v: S \to \mathbb{R} $ is the expected return at state $ s $:
\begin{equation*}
\begin{split}
v(s) & = \text{E}(G_t | S_t = s) \\
 & = \text{E}(R_{t+1} + \gamma G_{t+1} | S_t = s)
\end{split}
\end{equation*}
From the law of total expectation, $ \text{E}(X) = \text{E}(\text{E}(X|Y)) $:
\begin{equation*}
\begin{split}
\text{E}(G_{t+1}) & = \text{E}(\text{E}(G_{t+1}|S_{t+1} = s')) \\
& = \text{E}(v(s'))
\end{split}
\end{equation*}
The Bellman expectation equation for the state value function $ v(s) $ is:
\begin{equation*}
\begin{split}
v(s) & = \text{E}(R_{t+1} + \gamma v(s') | S_t = s) \\
 & = \sum_{s' \in \mathcal{S}} P_{s,s'} r(s') + \gamma \sum_{s' \in \mathcal{S}} P_{s,s'} v(s') \\
 & = R_s + \gamma \sum_{s' \in \mathcal{S}} P_{s,s'} v(s')
\end{split}
\end{equation*}
% TODO: Add Bellman look-ahead diagram
% TODO: Derive Bellman expectation equation from diagram
% TODO: Add example state diagram and solve

\subsection{Markov decision process}
A Markov decision process is described by:
\begin{itemize}    
    \item a finite set of states $ \mathcal{S} $
    \item a finite set of rewards $ \mathcal{R} \subset \mathbb{R} $
    \item a finite set of actions $ \mathcal{A} $
    \item a state transition probability matrix $ P_{s,s'}^a $
    \item a policy distribution $ \pi \in [0,1] $
    \item a reward function $r: \mathcal{S} \to \mathcal{R} $
    \item a discount factor $ \gamma \in [0, 1] $
\end{itemize}
The state transition probability matrix $ P_{s,s'}^a $ gives the probabilities of transitioning from state $ s $ to state $ s' $ if action $ a \in \mathcal{A} $ is taken:
\[ P_{s,s'}^a = \text{P}(S_{t+1} = s' | S_t = s, A_t = a) \]
The policy $ \pi $ maps a state $ s $ to the probability of selecting action $ a $:
\[ \pi(a | s) = \text{P}(A_t = a | S_t = s) \]
The action $ A_t $ is sampled from the policy:
\[ A_t \sim \pi( \cdot | S_t = s) \]
While following policy $ \pi $, the state transition probability matrix $ P_{s,s'}^{\pi} $ is:
\[ P_{s,s'}^{\pi} = \sum_{a \in \mathcal{A}} \pi(a | s) P_{s,s'}^a \]
The next expected reward $ R_s^a $ at state $ s $ given action $ a $ is taken is defined as:
\begin{equation*}
\begin{split}
R_s^a & = \text{E}(R_{t+1} | S_t = s, A_t = a) \\
 & = \sum_{s' \in \mathcal{S}} r(s') P_{s,s'}^a
\end{split}
\end{equation*}
While following policy $ \pi $, the next expected reward $ R_s^{\pi} $ is:
\[ R_s^{\pi} = \sum_{a \in \mathcal{A}} \pi(a | s) R_s^a \]
The state value function $ v_{\pi}: S \to \mathbb{R} $ is the expected return at state $ s $ under policy $ \pi $:
\begin{equation*}
\begin{split}
v_{\pi}(s) & = \text{E}_{\pi}(G_t | S_t = s) \\
 & = \text{E}_{\pi}(R_{t+1} + \gamma G_{t+1} | S_t = s)
\end{split}
\end{equation*}
From the law of total expectation, $ \text{E}(X) = \text{E}(\text{E}(X|Y)) $:
\begin{equation*}
\begin{split}
\text{E}_{\pi}(G_{t+1}) & = \text{E}_{\pi}(\text{E}_{\pi}(G_{t+1}|S_{t+1} = s')) \\
& = \text{E}_{\pi}(v_{\pi}(s'))
\end{split}
\end{equation*}
The Bellman expectation equation for the state value function $ v_{\pi}(s) $ under policy $\pi $ is:
\begin{equation*}
\begin{split}
v_{\pi}(s) & = \text{E}_{\pi}(R_{t+1} + \gamma v_{\pi}(s') | S_t = s) \\
 & = \text{E}_{\pi}(R_{t+1} | S_t = s) + \gamma \text{E}_{\pi}(v_{\pi}(s') | S_t = s) \\
 & = \sum_{a \in \mathcal{A}} \pi(a | s) R_s^a + \gamma \sum_{a \in \mathcal{A}} \pi(a | s) \sum_{s' \in \mathcal{S}} P_{s,s'}^a v_{\pi}(s') \\
 & = R_s^{\pi} + \gamma \sum_{a \in \mathcal{A}} \pi(a | s) \sum_{s' \in \mathcal{S}} P_{s,s'}^a v_{\pi}(s')
\end{split}
\end{equation*}
The action-value function $ q_{\pi}: \mathcal{S} \times \mathcal{A} \to \mathbb{R} $ is the expected return at state $ s $, taking action $ a $ and following policy $ \pi $:
\begin{equation*}
\begin{split}
q_{\pi}(s,a) & = \text{E}_{\pi}(G_t | S_t = s, A_t = a) \\
 & = \text{E}_{\pi}(R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a)
\end{split}
\end{equation*}
From the law of total expectation, $ \text{E}(X) = \text{E}(\text{E}(X|Y,Z)) $:
\begin{equation*}
\begin{split}
\text{E}_{\pi}(G_{t+1}) & = \text{E}_{\pi}(\text{E}_{\pi}(G_{t+1}|S_{t+1} = s', A_{t+1} = a')) \\
& = \text{E}_{\pi}(q_{\pi}(s',a'))
\end{split}
\end{equation*}
The Bellman expectation equation for the action-state value function $ q_{\pi}(s,a) $ under policy $\pi $ is:
\begin{equation*}
\begin{split}
q_{\pi}(s,a) & = \text{E}_{\pi}(R_{t+1} + \gamma q_{\pi}(s',a') | S_t = s, A_t = a) \\
 & = \text{E}_{\pi}(R_{t+1} | S_t = s, A_t = a) + \gamma \text{E}_{\pi}(q_{\pi}(s',a') | S_t = s, A_t = a) \\
 & = R_s^a + \gamma \sum_{s' \in \mathcal{S}} P_{s,s'}^a \sum_{a' \in \mathcal{A}} \pi(a' | s') q_{\pi}(s',a')
\end{split}
\end{equation*}
The state value function $ v_{\pi} $ can be written in terms of the action-state value function $ q_{\pi} $:
\[ v_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a | s)q_{\pi}(s,a) \]
Similarly, the action-state value function $ q_{\pi} $ can be written in terms of the state value function $ v_{\pi} $:
\[ q_{\pi}(s,a) = R_s^a + \gamma \sum_{s' \in \mathcal{S}} P_{s,s'}^a v_{\pi}(s) \]
% TODO: Add Bellman look-ahead diagrams
% TODO: Derive Bellman expectation equations from diagrams
% TODO: Add example state diagram and solve

\section{Model-Based Policy Optimisation}

\subsection{Policy Evaluation}

% TODO: Bellman optimality equations
% TODO: Full Bellman look-ahead diagrams
% TODO: Derive Bellman optimality equations from diagrams

\subsection{Policy Iteration}

\subsection{Value Iteration}

\section{Model-Free Policy Evaluation}

\subsection{Monte Carlo Learning}

\subsection{Temporal Difference Learning}

\section{Model-Free Policy Optimisation}

\section{Value Approximation}

\section{Policy Gradient Methods}

\section{Actor-Critic Methods}

%\begin{tikzpicture}
%    \draw (2.5,-2) circle [radius=0.5];
%    \draw (2.5,-6) circle [radius=0.5];
%    \draw (2.5,-10) circle [radius=0.5];
%    \draw (7.5,-6) circle [radius=0.5];
%    \draw (9.5,-6.5) rectangle (10.5,-5.5);
%    \node at (0.2,-2) {$ x_1 $};
%    \node at (0.2,-6) {$ x_2 $};
%    \node at (0.2,-10) {$ x_n $};
%    \node at (0.2,-8) {$ \vdots $};
%    \node at (2.5,0) {$ w_1 $};
%    \node at (2.5,-4) {$ w_2 $};
%    \node at (2.5,-8) {$ w_n $};
%    \node at (2.5,-2) {$ \times $};
%    \node at (2.5,-6) {$ \times $};
%    \node at (2.5,-10) {$ \times $};
%    \node at (7.5,-6) {$ + $};
%    \node at (7.5,-4) {$ w_0 $};
%    \node at (10,-6) {$ \phi(\cdot) $};
%    \node at (12.2,-6) {$ y $};
%    \draw (0.5,-2) -- (2,-2);
%    \draw (0.5,-6) -- (2,-6);
%    \draw (0.5,-10) -- (2,-10);
%    \draw (2.5,-0.2) -- (2.5,-1.5);
%    \draw (2.5,-4.2) -- (2.5,-5.5);
%    \draw (2.5,-8.2) -- (2.5,-9.5);
%    \draw (3,-2) -- (4.5,-2);
%    \draw (3,-6) -- (7,-6);
%    \draw (3,-10) -- (4.5,-10);
%    \draw (4.5,-2) -- (7.14645,-5.64645);
%    \draw (4.5,-10) -- (7.14645,-6.35355);
%    \draw (7.5,-4.2) -- (7.5,-5.5);
%    \draw (8,-6) -- (9.5,-6);
%    \draw (10.5,-6) -- (12,-6);
%\end{tikzpicture}

\end{document}